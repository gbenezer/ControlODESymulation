{
  "hash": "9b93d9f6d4ec659ddbfb65774c4b8e44",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Integration Framework Architecture\"\nsubtitle: \"Multi-Backend Integration for ODEs and SDEs\"\nauthor: \"Gil Benezer\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 5\n    code-fold: show\n    code-tools: true\n    theme: cosmo\nexecute:\n  eval: true\n  cache: true\n  warning: false\n---\n\n## Overview {#sec-overview}\n\nThe ControlDESymulation numerical integration framework provides **multi-backend**, **multi-method** support for integrating both **deterministic ordinary differential equations (ODEs)** and **stochastic differential equations (SDEs)**. The framework consists of 14 core files organized into a clean two-track architecture with a shared method registry.\n\n**Key capabilities:**\n\n- **Backend agnostic**: Seamless switching between NumPy, PyTorch, and JAX\n- **40+ integration methods**: From simple Euler to high-order adaptive schemes\n- **Stochastic support**: Full SDE integration with noise structure exploitation\n- **Factory pattern**: Automatic method selection based on system properties\n- **Production ready**: Professional-grade error control and performance tracking\n\n::: {.callout-important}\n## User Interface\n\n**Most users should NOT directly instantiate integrators or factories.** Instead, use the high-level system interface:\n\n```python\n# Recommended: Use system.integrate() - delegates to appropriate integrator\nsystem = Pendulum(m_val=1.0, l_val=0.5)\nresult = system.integrate(\n    x0=np.array([1.0, 0.0]),\n    method='RK45',  # Optional: framework selects automatically\n    t_span=(0, 10)\n)\n\n# Advanced: Direct integrator access (only when needed for fine control)\nintegrator = IntegratorFactory.auto(system)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n```\n\nThe system's `integrate()` method automatically handles backend selection, method routing, and integrator lifecycle. Direct integrator instantiation is only needed for advanced use cases requiring explicit integrator reuse or fine-grained control.\n:::\n\n## Architecture Overview {#sec-architecture}\n\nThe framework follows a dual-track design separating deterministic and stochastic integration, with a shared **method registry** providing centralized method management:\n\n```\nShared Infrastructure\n├── MethodRegistry (centralized method normalization and validation)\n│   ├── normalize_method_name() - cross-backend name mapping\n│   ├── validate_method() - backend/system compatibility\n│   ├── is_sde_method() / is_fixed_step() - classification\n│   └── get_available_methods() - method discovery\n│\nTrack 1: Deterministic ODE Integration\n├── IntegratorBase (abstract interface)\n├── IntegratorFactory (method selection via registry)\n├── Backend-Specific Implementations:\n│   ├── ScipyIntegrator (NumPy/SciPy)\n│   ├── TorchDiffEqIntegrator (PyTorch with GPU)\n│   ├── DiffraxIntegrator (JAX with XLA)\n│   └── DiffEqPyIntegrator (Julia via Python)\n└── FixedStepIntegrators (Euler, Heun, Midpoint, RK4)\n\nTrack 2: Stochastic SDE Integration\n├── SDEIntegratorBase (extends IntegratorBase)\n├── SDEIntegratorFactory (SDE-specific creation via registry)\n├── Backend-Specific Implementations:\n│   ├── TorchSDEIntegrator (PyTorch SDEs)\n│   ├── DiffraxSDEIntegrator (JAX SDEs)\n│   └── DiffEqPySDEIntegrator (Julia SDEs)\n└── CustomBrownianPath (deterministic noise for testing)\n```\n\n\n\n## Method Registry: Centralized Method Management {#sec-method-registry}\n\n**File:** `method_registry.py`\n\nThe `method_registry` module serves as the **single source of truth** for all integration methods across backends. It provides centralized method normalization, validation, and classification for both ODE and SDE integrators.\n\n### Design Philosophy\n\n**Backend Naming Conventions:**\n\n| Backend | Convention | Examples |\n|---------|------------|----------|\n| NumPy/Julia (DiffEqPy) | Capitalized | `EM`, `Tsit5`, `SRIW1` |\n| PyTorch (TorchSDE/TorchDiffEq) | lowercase | `euler`, `dopri5`, `milstein` |\n| JAX (Diffrax) | PascalCase | `Euler`, `ItoMilstein`, `Tsit5` |\n\n**Canonical Names:**\n\nUser-friendly aliases that work across all backends (e.g., `euler_maruyama`, `milstein`, `rk45`). These are automatically normalized to backend-specific names.\n\n### Core Functions\n\n```python\nfrom cdesym.systems.base.numerical_integration.method_registry import (\n    normalize_method_name,\n    validate_method,\n    is_sde_method,\n    is_fixed_step,\n    get_available_methods,\n    get_method_info,\n)\n\n# Normalize canonical names to backend-specific\nnormalize_method_name('euler_maruyama', 'numpy')  # → 'EM'\nnormalize_method_name('euler_maruyama', 'torch')  # → 'euler'\nnormalize_method_name('euler_maruyama', 'jax')    # → 'Euler'\n\n# Validate method/backend/system compatibility\nis_valid, error = validate_method('euler_maruyama', 'torch', is_stochastic=True)\n\n# Classify methods\nis_sde_method('euler_maruyama')  # → True\nis_sde_method('rk4')              # → False\nis_fixed_step('rk4')              # → True\nis_fixed_step('RK45')             # → False (adaptive)\n\n# Discover available methods\nmethods = get_available_methods('torch', method_type='stochastic')\n```\n\n### Method Classification\n\nMethods are classified along two dimensions:\n\n1. **System type**: Deterministic (ODE) vs Stochastic (SDE)\n2. **Time stepping**: Fixed-step vs Adaptive\n\n| Category | Examples | Use Case |\n|----------|----------|----------|\n| `DETERMINISTIC_FIXED_STEP` | `euler`, `heun`, `midpoint`, `rk4` | Manual implementations |\n| `DETERMINISTIC_ADAPTIVE` | `RK45`, `LSODA`, `dopri5`, `tsit5` | Production ODE solvers |\n| `SDE_FIXED_STEP` | `EM`, `euler`, `milstein`, `SRIW1` | Most SDE methods |\n| `SDE_ADAPTIVE` | `LambaEM`, `AutoEM`, `adaptive_heun` | Rare adaptive SDE |\n\n### Normalization Map\n\nThe registry maintains a comprehensive map for cross-backend method translation:\n\n```python\nNORMALIZATION_MAP = {\n    # SDE Methods\n    'euler_maruyama': {'numpy': 'EM', 'torch': 'euler', 'jax': 'Euler'},\n    'milstein': {'numpy': 'RKMil', 'torch': 'milstein', 'jax': 'ItoMilstein'},\n    'sra1': {'numpy': 'SRA1', 'torch': 'srk', 'jax': 'SRA1'},\n\n    # ODE Methods\n    'rk45': {'numpy': 'RK45', 'torch': 'dopri5', 'jax': 'tsit5'},\n    'dopri5': {'numpy': 'RK45', 'torch': 'dopri5', 'jax': 'dopri5'},\n    'tsit5': {'numpy': 'Tsit5', 'torch': 'dopri5', 'jax': 'tsit5'},\n    # ... more mappings\n}\n```\n\n### Usage in Factories\n\nBoth `IntegratorFactory` and `SDEIntegratorFactory` use the registry for method handling:\n\n```python\n# IntegratorFactory.create() internally:\nmethod = normalize_method_name(method, backend)\nis_valid, error = validate_method(method, backend, is_stochastic=False)\nif is_fixed_step(method) and dt is None:\n    raise ValueError(\"Fixed-step method requires dt\")\n```\n\n## Track 1: Deterministic ODE Integration {#sec-track-1}\n\n### IntegratorBase: Abstract Interface {#sec-integratorbase}\n\n**File:** `integrator_base.py`\n\nThe `IntegratorBase` class defines the unified interface that all numerical integrators must implement. This abstraction enables backend-agnostic integration while maintaining consistent behavior across implementations.\n\n**Core responsibilities:**\n\n- Define integration contract through abstract methods\n- Track performance statistics (function evaluations, steps, timing)\n- Manage integration parameters (time step, tolerances, backend)\n- Provide consistent result format via `IntegrationResult` TypedDict\n\n**Key attributes:**\n\n```python\nsystem: ContinuousSystemBase     # Dynamical system to integrate\ndt: float                        # Time step (or initial guess for adaptive)\nstep_mode: StepMode              # FIXED or ADAPTIVE\nbackend: Backend                 # 'numpy', 'torch', or 'jax'\nrtol: float                      # Relative error tolerance (adaptive methods)\natol: float                      # Absolute error tolerance (adaptive methods)\n_stats: dict                     # Performance tracking\n```\n\n**Abstract methods required by all implementations:**\n\n```python\ndef step(self, x: Array, u: Array, dt: float) -> Array:\n    \"\"\"Single integration step: x(t) -> x(t + dt)\"\"\"\n    \ndef integrate(\n    self, \n    x0: Array, \n    u_func: Callable, \n    t_span: tuple[float, float]\n) -> IntegrationResult:\n    \"\"\"Multi-step integration over time interval\"\"\"\n    \n@property\ndef name(self) -> str:\n    \"\"\"Unique identifier for this integrator\"\"\"\n```\n\n**StepMode enumeration:**\n\n- `FIXED`: Constant time step (euler, rk4, midpoint)\n- `ADAPTIVE`: Variable time step with error control (RK45, dopri5, tsit5)\n\n### IntegratorFactory: Smart Creation {#sec-integratorfactory}\n\n**File:** `integrator_factory.py`\n\nThe `IntegratorFactory` provides intelligent integrator creation with automatic backend and method selection. It encapsulates the complexity of choosing appropriate integration methods based on system properties and use case requirements.\n\n**Integration with Method Registry:**\n\nThe factory delegates method normalization and validation to the centralized `method_registry`:\n\n```python\n# Internally, IntegratorFactory.create() does:\nmethod = normalize_method_name(method, backend)  # Registry function\nis_valid, error = validate_method(method, backend, is_stochastic=False)\nif is_fixed_step(method) and dt is None:\n    raise ValueError(\"Fixed-step method requires dt\")\n```\n\n**Julia Preference for NumPy Backend:**\n\nWhen using the NumPy backend, `IntegratorFactory` automatically prefers Julia implementations for basic methods when DiffEqPy is available:\n\n- `euler` → Julia's `Euler` (better performance)\n- `heun` → Julia's `Heun`\n- `midpoint` → Julia's `Midpoint`\n\nTo explicitly use manual Python implementations:\n```python\nintegrator = IntegratorFactory.create(\n    system, backend='numpy', method='euler', prefer_manual=True\n)\n```\n\n**Factory methods:**\n\n```python\n# Automatic selection based on system and backend\nIntegratorFactory.auto(system, backend=None)\n\n# Production use with auto-stiffness detection\nIntegratorFactory.for_production(system, **options)\n\n# High-performance Julia integration\nIntegratorFactory.for_julia(system, algorithm='Tsit5', **options)\n\n# Neural ODE training with adjoint gradients\nIntegratorFactory.for_neural_ode(system, **options)\n\n# JAX optimization workflows\nIntegratorFactory.for_optimization(system, **options)\n\n# Direct method specification\nIntegratorFactory.create(system, backend, method, **options)\n```\n\n**Method registry:**\n\nThe factory uses the centralized method registry for mapping method names to backends and capabilities:\n\n| Method | Backend | Type | Order | Best For |\n|--------|---------|------|-------|----------|\n| LSODA | NumPy (scipy) | Adaptive | Variable | Automatic stiffness detection |\n| RK45 | NumPy (scipy) | Adaptive | 5(4) | General non-stiff ODEs |\n| DOP853 | NumPy (scipy) | Adaptive | 8(5,3) | High-accuracy requirements |\n| Radau | NumPy (scipy) | Adaptive | 5 | Stiff systems (implicit) |\n| BDF | NumPy (scipy) | Adaptive | Variable | Very stiff systems |\n| Tsit5 | NumPy (Julia) | Adaptive | 5(4) | High performance |\n| Vern9 | NumPy (Julia) | Adaptive | 9(8) | Maximum accuracy |\n| Rodas5 | NumPy (Julia) | Adaptive | 5(4) | Stiff (Rosenbrock method) |\n| dopri5 | PyTorch/JAX | Adaptive | 5(4) | Neural ODEs, GPU acceleration |\n| dopri8 | PyTorch/JAX | Adaptive | 8 | High-accuracy neural ODEs |\n| tsit5 | JAX (diffrax) | Adaptive | 5(4) | Optimization, XLA compilation |\n| euler | Any | Fixed | 1 | Simple systems, education |\n| heun | Any | Fixed | 2 | Improved Euler, predictor-corrector |\n| midpoint | Any | Fixed | 2 | Second-order, midpoint evaluation |\n| rk4 | Any | Fixed | 4 | Moderate accuracy, fixed step |\n\n### ScipyIntegrator: Adaptive NumPy Integration {#sec-scipyintegrator}\n\n**File:** `scipy_integrator.py`\n\nWraps `scipy.integrate.solve_ivp` to provide professional-grade adaptive integration with comprehensive error control. This is the recommended starting point for most NumPy-based applications.\n\n**Supported methods:**\n\n- **RK45** (default): Dormand-Prince 5(4) — general purpose, good balance\n- **RK23**: Bogacki-Shampine 3(2) — fast, lower accuracy\n- **DOP853**: Dormand-Prince 8(5,3) — very high accuracy\n- **Radau**: Implicit Runge-Kutta — stiff systems\n- **BDF**: Backward differentiation formulas — very stiff systems\n- **LSODA**: Automatic stiffness detection — adapts to problem\n\n**Key features:**\n\n- Professional adaptive time stepping with embedded error estimation\n- Configurable error tolerances (`rtol`, `atol`)\n- Dense output via continuous extension (interpolation between steps)\n- Event detection for state-dependent conditions\n- Support for both controlled and autonomous systems\n\n**Usage example:**\n\n::: {#ex-scipy .cell execution_count=2}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.scipy_integrator import ScipyIntegrator\n\n# Create system\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Create adaptive integrator with tight tolerances\nintegrator = ScipyIntegrator(\n    system,\n    method='RK45',\n    rtol=1e-6,\n    atol=1e-8\n)\n\n# Integrate over time span\nx0 = np.array([1.0, 0.0])  # [angle, angular_velocity]\nu_func = lambda t, x: np.zeros(1)  # No control input\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Success: {result['success']}\")\nprint(f\"Function evaluations: {result['nfev']}\")\nprint(f\"Steps taken: {result['nsteps']}\")\n```\n:::\n\n\n**When to use:**\n\n- General-purpose NumPy applications\n- When you need reliable adaptive stepping\n- Systems with unknown stiffness (use LSODA)\n- When dense output is required\n\n### TorchDiffEqIntegrator: GPU-Accelerated PyTorch {#sec-torchdiffeqintegrator}\n\n**File:** `torchdiffeq_integrator.py**\n\nProvides PyTorch integration with GPU acceleration and automatic differentiation support. Essential for neural ODE applications and gradient-based optimization.\n\n**Supported methods:**\n\n- **dopri5**: Dormand-Prince 5(4) — recommended default\n- **dopri8**: Dormand-Prince 8 — high accuracy\n- **adaptive_heun**: Heun's method — good for moderately stiff\n- **bosh3**: Bogacki-Shampine 3 — fast, lower accuracy\n- **fehlberg2**: Fehlberg 2(1) — very fast, low accuracy\n- **explicit_adams**, **implicit_adams**: Multi-step methods\n\n**Key features:**\n\n- **GPU acceleration**: Automatic CUDA support for large-scale problems\n- **Automatic differentiation**: Seamless gradient computation through dynamics\n- **Adjoint method**: Memory-efficient backpropagation for neural ODEs\n- **Batch processing**: Vectorized integration of multiple trajectories\n\n**Usage example:**\n\n::: {#ex-torch .cell execution_count=3}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.torchdiffeq_integrator import TorchDiffEqIntegrator\n\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Create GPU-accelerated integrator\nintegrator = TorchDiffEqIntegrator(\n    system,\n    method='dopri5',\n    backend='torch',\n    device='cpu',  # Use 'cuda:0' for GPU\n    rtol=1e-6,\n    atol=1e-8\n)\n\nx0 = torch.tensor([1.0, 0.0], requires_grad=True)\nu_func = lambda t, x: torch.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Solver: {result['solver']}\")\nprint(f\"Integration time: {result['integration_time']:.4f}s\")\n```\n:::\n\n\n**When to use:**\n\n- Neural ODE training\n- GPU-accelerated simulations\n- Gradient-based optimization\n- Large-scale batch processing\n\n### DiffraxIntegrator: JAX with XLA Compilation {#sec-diffraxintegrator}\n\n**File:** `diffrax_integrator.py`\n\nLeverages JAX's diffrax library for high-performance integration with XLA compilation. Ideal for optimization workflows requiring extensive JIT compilation and functional transformations.\n\n**Supported methods:**\n\n- **tsit5**: Tsitouras 5(4) — recommended, efficient\n- **dopri5**: Dormand-Prince 5(4) — standard reference\n- **dopri8**: Dormand-Prince 8 — high accuracy\n- **heun**: Heun's method — simple, robust\n- **ralston**: Ralston's method — improved stability\n- **reversible_heun**: Time-reversible integration\n\n**Key features:**\n\n- **XLA compilation**: Near-C++ performance via just-in-time compilation\n- **JAX transformations**: `jit`, `vmap`, `grad`, `pmap` all work seamlessly\n- **Functional style**: Pure functions enable advanced optimizations\n- **Efficient for optimization**: Fast repeated evaluations with parameter sweeps\n\n**Usage example:**\n\n::: {#ex-jax .cell execution_count=4}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.diffrax_integrator import DiffraxIntegrator\n\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Create JAX integrator\nintegrator = DiffraxIntegrator(\n    system,\n    method='tsit5',\n    backend='jax',\n    rtol=1e-6,\n    atol=1e-8\n)\n\nx0 = jnp.array([1.0, 0.0])\nu_func = lambda t, x: jnp.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Method: {result['solver']}\")\nprint(f\"Success: {result['success']}\")\n```\n:::\n\n\n**When to use:**\n\n- Parameter optimization requiring many integrations\n- When you need `vmap` for batch parameter sweeps\n- Gradient-based control optimization\n- Maximum computational efficiency\n\n### DiffEqPyIntegrator: Julia's Solver Ecosystem {#sec-diffeqpyintegrator}\n\n**File:** `diffeqpy_integrator.py`\n\nAccesses Julia's extensive DifferentialEquations.jl ecosystem through Python bindings. Provides the most comprehensive method library with production-grade performance.\n\n**Supported method families:**\n\n**Explicit Runge-Kutta:**\n- Tsit5, Vern6, Vern7, Vern8, Vern9 — variable order adaptive\n- DP5, DP8 — Dormand-Prince variants\n\n**Rosenbrock (implicit for stiff):**\n- Rosenbrock23, Rosenbrock32, Rodas4, Rodas5\n\n**BDF methods:**\n- TRBDF2, KenCarp3, KenCarp4, KenCarp5\n\n**Specialized:**\n- RadauIIA5 — implicit Runge-Kutta\n- ROCK2, ROCK4 — stabilized methods\n- VelocityVerlet, SymplecticEuler — symplectic integrators\n\n**Auto-switching (composite):**\n- AutoTsit5(Rosenbrock23()) — switches based on stiffness\n- AutoVern7(Rodas5()) — high accuracy with stiffness handling\n\n**Key features:**\n\n- **Highest performance**: Often 2-10× faster than SciPy\n- **Automatic stiffness detection**: Seamlessly switches solvers\n- **Extensive method library**: 100+ algorithms available\n- **Production reliability**: Battle-tested in scientific computing\n\n**Usage example:**\n\n::: {#ex-julia .cell execution_count=5}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.diffeqpy_integrator import DiffEqPyIntegrator\n\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Ultra-high accuracy Julia integration\nintegrator = DiffEqPyIntegrator(\n    system,\n    algorithm='Vern9',  # 9th order method\n    backend='numpy',\n    reltol=1e-12,\n    abstol=1e-14\n)\n\nx0 = np.array([1.0, 0.0])\nu_func = lambda t, x: np.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 100))\n\nprint(f\"Function evaluations: {result['nfev']}\")\nprint(f\"Achieved accuracy: reltol={1e-12}, abstol={1e-14}\")\n```\n:::\n\n\n**When to use:**\n\n- Maximum performance requirements\n- Very stiff systems needing automatic detection\n- High-accuracy applications (>8 digits)\n- Production environments with reliability requirements\n\n### FixedStepIntegrators: Educational and Simple Systems {#sec-fixedstepintegrators}\n\n**File:** `fixed_step_integrators.py`\n\nManual implementations of classic fixed-step methods. These provide transparent, backend-agnostic integration suitable for education and simple systems where adaptive stepping is unnecessary.\n\n**Available methods:**\n\n- **ExplicitEulerIntegrator**: Forward Euler (order 1) — simplest method\n- **HeunIntegrator**: Heun's method / Improved Euler (order 2) — predictor-corrector approach\n- **MidpointIntegrator**: Midpoint method (order 2) — midpoint evaluation\n- **RK4Integrator**: Classic Runge-Kutta 4 (order 4) — excellent balance\n\n**HeunIntegrator:**\n\nHeun's method uses a predictor-corrector approach with trapezoidal averaging:\n\n```\nk1 = f(x_k, u_k)                    # Predictor (Euler step)\nx_pred = x_k + dt * k1\nk2 = f(x_pred, u_k)                 # Corrector\nx_{k+1} = x_k + (dt/2) * (k1 + k2)  # Trapezoidal rule\n```\n\nAlso known as Improved Euler or Explicit Trapezoid method. Uses 2 function evaluations per step for second-order accuracy.\n\n**Key features:**\n\n- **Backend agnostic**: Work with NumPy, PyTorch, and JAX arrays\n- **Transparent implementation**: Clear, readable code for learning\n- **Constant time step**: Predictable computational cost\n- **No external dependencies**: Pure Python implementations\n- **TypedDict results**: All integrators return `IntegrationResult` TypedDict\n\n**Usage example:**\n\n::: {#ex-fixed .cell execution_count=6}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.fixed_step_integrators import (\n    RK4Integrator,\n    HeunIntegrator,\n)\n\nsystem = Pendulum(m_val=1.0, l_val=0.5)\nx0 = np.array([1.0, 0.0])\nu_func = lambda t, x: np.zeros(1)\n\n# RK4 with fixed time step (4 function evals/step)\nrk4_integrator = RK4Integrator(system, dt=0.01, backend='numpy')\nresult_rk4 = rk4_integrator.integrate(x0, u_func, t_span=(0, 10))\n\n# Heun's method (2 function evals/step, predictor-corrector)\nheun_integrator = HeunIntegrator(system, dt=0.01, backend='numpy')\nresult_heun = heun_integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"RK4 - Steps: {result_rk4['nsteps']}, Solver: {result_rk4['solver']}\")\nprint(f\"Heun - Steps: {result_heun['nsteps']}, Solver: {result_heun['solver']}\")\n```\n:::\n\n\n**When to use:**\n\n- Educational purposes and learning\n- Simple systems with smooth dynamics\n- Real-time applications requiring predictable timing\n- Debugging with deterministic stepping\n\n## Track 2: Stochastic SDE Integration {#sec-track-2}\n\n### SDEIntegratorBase: Stochastic Abstract Interface {#sec-sdeintegratorbase}\n\n**File:** `sde_integrator_base.py`\n\nExtends `IntegratorBase` to handle stochastic differential equations of the form:\n\n$$dx = f(x, u, t)dt + g(x, u, t)dW$$\n\nwhere:\n\n- $f(x, u, t)$: Drift term (deterministic dynamics)\n- $g(x, u, t)$: Diffusion term (stochastic intensity)\n- $dW$: Brownian motion increments (Wiener process)\n\n**Key differences from deterministic integration:**\n\n1. **Random noise generation**: Requires proper Brownian motion sampling\n2. **Convergence types**: \n   - **Strong convergence**: Pathwise accuracy (Monte Carlo, control)\n   - **Weak convergence**: Distribution accuracy (statistics)\n3. **Noise structure exploitation**: Additive, diagonal, scalar, or general\n4. **Multiple realizations**: Monte Carlo simulation support\n5. **Interpretation**: Itô vs Stratonovich calculus\n\n**Additional abstract methods:**\n\n```python\ndef step(\n    self, \n    x: Array, \n    u: Array, \n    dt: float, \n    dW: Array\n) -> Array:\n    \"\"\"Single SDE step with provided noise\"\"\"\n    \ndef integrate_monte_carlo(\n    self,\n    x0: Array,\n    u_func: Callable,\n    t_span: tuple[float, float],\n    n_paths: int\n) -> SDEIntegrationResult:\n    \"\"\"Multiple trajectory simulation for statistics\"\"\"\n```\n\n### SDEIntegratorFactory: SDE-Specific Creation {#sec-sdeintegratorfactory}\n\n**File:** `sde_integrator_factory.py`\n\nProvides intelligent SDE integrator creation with automatic noise structure detection and method selection.\n\n**Integration with Method Registry:**\n\nLike `IntegratorFactory`, the SDE factory delegates to the centralized `method_registry` for method normalization and validation:\n\n```python\n# Internally, SDEIntegratorFactory.create() does:\nmethod = normalize_method_name(method, backend)  # Registry function\nis_valid, error = validate_method(method, backend, is_stochastic=True)\nif is_fixed_step(method) and dt is None:\n    raise ValueError(\"Fixed-step SDE method requires dt\")\n```\n\n**Factory methods:**\n\n```python\n# Automatic selection based on noise structure\nSDEIntegratorFactory.auto(sde_system, backend=None)\n\n# Direct method specification\nSDEIntegratorFactory.create(\n    sde_system, backend, method, dt, **options\n)\n\n# Optimized for Monte Carlo simulations\nSDEIntegratorFactory.for_monte_carlo(\n    sde_system, noise_type='general', **options\n)\n\n# Neural SDE training with adjoint\nSDEIntegratorFactory.for_neural_sde(sde_system, adjoint=True, **options)\n\n# Julia DiffEqPy SDE solvers\nSDEIntegratorFactory.for_julia(sde_system, algorithm='SRIW1', **options)\n\n# Gradient-based optimization\nSDEIntegratorFactory.for_optimization(sde_system, backend=None, **options)\n```\n\n**Available SDE methods:**\n\n| Method | Backend | Convergence | Noise Type | Order |\n|--------|---------|-------------|------------|-------|\n| euler-maruyama | All | Strong 0.5 | General | 0.5 |\n| milstein | PyTorch/NumPy | Strong 1.0 | Diagonal | 1.0 |\n| reversible_heun | PyTorch/JAX | Strong 1.0 | Additive | 1.0 |\n| adaptive_heun | PyTorch | Strong 1.0 | Additive | 1.0 (adaptive) |\n| srk | PyTorch | Strong | General | Variable |\n| midpoint | PyTorch | Strong | General | Variable |\n\n**Noise structure types:**\n\n- **Additive**: $g(x, u, t) = g(t)$ — diffusion independent of state\n- **Diagonal**: $g$ is diagonal matrix — independent noise channels\n- **Scalar**: $g$ is scalar — single noise source\n- **General**: Full matrix $g$ — correlated noise\n\n### TorchSDEIntegrator: PyTorch SDE Integration {#sec-torchsdeintegrator}\n\n**File:** `torchsde_integrator.py`\n\nGPU-accelerated SDE integration using the `torchsde` library. Supports automatic differentiation through stochastic dynamics.\n\n**Supported methods:**\n\n- **euler**: Euler-Maruyama (strong order 0.5) — Itô and Stratonovich\n- **milstein**: Milstein method (strong order 1.0 for diagonal noise) — Itô only\n- **srk**: Stochastic Runge-Kutta (general noise) — Itô and Stratonovich\n- **midpoint**: Midpoint method — Stratonovich only\n- **reversible_heun**: Reversible Heun (strong order 1.0 for additive noise) — Stratonovich only\n- **adaptive_heun**: Adaptive Heun with error control — Itô and Stratonovich\n\n**Key features:**\n\n- GPU acceleration for large-scale stochastic simulations\n- Adaptive stepping with stochastic error control\n- Noise structure exploitation for efficiency\n- Adjoint method for memory-efficient gradients through SDEs\n- **SDE type compatibility**: Most methods support either Itô or Stratonovich (see method list)\n\n**Usage example:**\n\n::: {#ex-torchsde .cell execution_count=7}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.stochastic.torchsde_integrator import TorchSDEIntegrator\n\n# Create Ornstein-Uhlenbeck process: dx = -α*x*dt + σ*dW\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\n\nintegrator = TorchSDEIntegrator(\n    sde_system,\n    method='euler',  # Euler-Maruyama for Itô SDEs\n    dt=0.01,\n    backend='torch'\n)\n\nx0 = torch.tensor([0.5])\nu_func = lambda t, x: torch.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Convergence type: {result['convergence_type']}\")\nprint(f\"SDE type: {result['sde_type']}\")\n```\n:::\n\n\n### DiffraxSDEIntegrator: JAX SDE Integration {#sec-diffraxsdeintegrator}\n\n**File:** `diffrax_sde_integrator.py`\n\nJAX-based SDE integration with XLA compilation and functional transformations. Excellent for optimization involving stochastic dynamics.\n\n**Supported methods:**\n\n- **euler**: Euler-Maruyama\n- **heun**: Heun's method (if supported by diffrax)\n- **reversible_heun**: Time-reversible stochastic integration\n\n**Key features:**\n\n- XLA compilation for near-C++ performance\n- JAX transformations (`jit`, `vmap`, `grad`) work seamlessly\n- Custom noise support for deterministic testing\n- Efficient for stochastic optimization\n\n**Usage example:**\n\n::: {#ex-diffraxsde .cell execution_count=8}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.stochastic.diffrax_sde_integrator import DiffraxSDEIntegrator\n\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\n\nintegrator = DiffraxSDEIntegrator(\n    sde_system,\n    method='euler',\n    dt=0.01,\n    backend='jax',\n    seed=SEED\n)\n\nx0 = jnp.array([0.5])\nu_func = lambda t, x: jnp.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Method: {result['solver']}\")\nprint(f\"Steps: {result['nsteps']}\")\n```\n:::\n\n\n### DiffEqPySDEIntegrator: Julia SDE Integration {#sec-diffeqpysdeintegrator}\n\n**File:** `diffeqpy_sde_integrator.py`\n\nAccess to Julia's comprehensive SDE solver ecosystem through Python bindings.\n\n**Supported methods:**\n\n- Euler-Maruyama variants\n- Milstein method\n- Stochastic Rosenbrock methods\n- Advanced Julia SDE algorithms\n\n**Key features:**\n\n- Production-grade SDE solvers\n- Automatic noise structure detection\n- High-performance algorithms\n- Extensive method library\n\n**Usage example:**\n\n::: {#ex-diffeqpysde .cell execution_count=9}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.stochastic.diffeqpy_sde_integrator import DiffEqPySDEIntegrator\n\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\n\nintegrator = DiffEqPySDEIntegrator(\n    sde_system,\n    method='EM',  # Euler-Maruyama\n    dt=0.01,\n    backend='numpy',\n    seed=SEED\n)\n\nx0 = np.array([0.5])\nu_func = lambda t, x: np.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n\nprint(f\"Diffusion evaluations: {result['diffusion_evals']}\")\n```\n:::\n\n\n### CustomBrownianPath: Deterministic Noise for Testing {#sec-custombrownianpath}\n\n**File:** `custom_brownian.py`\n\nProvides custom Brownian motion paths for deterministic testing and reproducibility. Implements diffrax's `AbstractPath` interface.\n\n**Key features:**\n\n- User-provided noise increments\n- Deterministic testing support\n- Custom noise patterns (e.g., zero noise, specific realizations)\n- Compatible with diffrax integrators\n\n**Usage example:**\n\n::: {#ex-custombrown .cell execution_count=10}\n``` {.python .cell-code}\nfrom jax import random\nfrom cdesym.systems.base.numerical_integration.stochastic.custom_brownian import (\n    CustomBrownianPath, create_custom_or_random_brownian\n)\nfrom cdesym.systems.base.numerical_integration.stochastic.diffrax_sde_integrator import DiffraxSDEIntegrator\n\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\nnw = sde_system.nw\n\n# Zero noise for deterministic testing\ndW = jnp.zeros((nw,))\nbrownian = CustomBrownianPath(t0=0.0, t1=10.0, dW=dW)\n\n# Or generate random noise\nkey = random.key(SEED)\nbrownian_random = create_custom_or_random_brownian(\n    key, t0=0.0, t1=10.0, shape=(nw,), dW=None\n)\n\n# Use in integration\nintegrator = DiffraxSDEIntegrator(sde_system, method='euler', dt=0.01, backend='jax')\nx0 = jnp.array([0.5])\nu_func = lambda t, x: jnp.zeros(1)\n\nresult = integrator.integrate(\n    x0, u_func, t_span=(0, 10), brownian_path=brownian\n)\n\nprint(\"Deterministic SDE integration (zero noise)\")\n```\n:::\n\n\n**Important notes:**\n\n1. **Time span matching**: `CustomBrownianPath(t0, t1, dW)` must match `t_span`\n2. **Single dW for entire integration**: Unlike `step()`, provide one `dW` for the full interval\n3. **Automatic interpolation**: Diffrax handles internal time queries\n\n## Integration Result Types {#sec-result-types}\n\n### IntegrationResult (Deterministic ODEs) {#sec-integrationresult}\n\nAll ODE integrators return a TypedDict with consistent fields:\n\n```python\n{\n    't': array,              # Time points (T,)\n    'x': array,              # States (T, nx) - time-major format\n    'success': bool,         # Integration succeeded\n    'message': str,          # Status message\n    'nfev': int,             # Function evaluations\n    'nsteps': int,           # Steps taken\n    'integration_time': float,  # Wall clock time (seconds)\n    'solver': str,           # Integrator name\n    \n    # Optional (adaptive methods only):\n    'njev': int,             # Jacobian evaluations\n    'nlu': int,              # LU decompositions\n    'status': int,           # Solver-specific status code\n    'sol': object,           # Dense output object (if requested)\n    'dense_output': bool,    # Dense output available\n}\n```\n\n### SDEIntegrationResult (Stochastic SDEs) {#sec-sdeintegrationresult}\n\nSDE integrators extend `IntegrationResult` with stochastic-specific fields:\n\n```python\n{\n    # All IntegrationResult fields, plus:\n    'diffusion_evals': int,     # Diffusion function calls\n    'noise_samples': array,     # Brownian increments used\n    'n_paths': int,             # Number of trajectories\n    'convergence_type': str,    # 'strong' or 'weak'\n    'sde_type': str,            # 'ito' or 'stratonovich'\n    'noise_type': str,          # 'additive', 'diagonal', 'scalar', 'general'\n    \n    # For Monte Carlo (n_paths > 1):\n    'x': array,                 # (n_paths, T, nx)\n    'statistics': dict,         # {'mean', 'std', 'q25', 'q50', 'q75'}\n}\n```\n\n## Practical Usage Examples {#sec-usage-examples}\n\n::: {.callout-note}\n## Recommended Usage Pattern\n\nThe examples below show **direct integrator instantiation for documentation purposes**. In practice, most users should use the system's `integrate()` method which delegates to the appropriate integrator automatically:\n\n```python\n# Recommended approach\nresult = system.integrate(x0=x0, method='RK45', t_span=(0, 10))\n\n# Advanced approach (shown in examples below)\nintegrator = IntegratorFactory.auto(system)\nresult = integrator.integrate(x0, u_func, t_span=(0, 10))\n```\n:::\n\n### Example 1: Recommended High-Level Interface\n\n::: {#ex-recommended .cell execution_count=11}\n``` {.python .cell-code}\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Most users should use this approach - system handles delegation\nx0 = np.array([0.1, 0.0])\nresult = system.integrate(\n    x0=x0,\n    method='RK45',  # Optional: auto-selected if not specified\n    t_span=(0.0, 10.0)\n)\n\nprint(f\"Method: {result['solver']}\")\nprint(f\"Steps: {result['nsteps']}\")\nprint(f\"Success: {result['success']}\")\n```\n:::\n\n\n### Example 2: Advanced - Direct Integrator Access\n\n::: {#ex-quickstart .cell execution_count=12}\n``` {.python .cell-code}\n# Advanced: Direct integrator instantiation for reuse or fine control\nsystem = Pendulum(m_val=1.0, l_val=0.5)\nintegrator = IntegratorFactory.auto(system)\n\n# Can reuse integrator for multiple integrations\nx0 = np.array([0.1, 0.0])\nu_func = lambda t, x: np.zeros(1)\nresult = integrator.integrate(x0, u_func, t_span=(0.0, 10.0))\n\nprint(f\"Method: {result['solver']}\")\nprint(f\"Steps: {result['nsteps']}\")\n```\n:::\n\n\n### Example 3: High-Accuracy Julia Integration\n\n::: {#ex-julia-accuracy .cell execution_count=13}\n``` {.python .cell-code}\n# Recommended: System-level interface\nsystem = Pendulum(m_val=1.0, l_val=0.5)\nx0 = np.array([1.0, 0.0])\n\nresult = system.integrate(\n    x0=x0,\n    method='Vern9',  # Julia's 9th order method\n    rtol=1e-12,\n    atol=1e-14,\n    t_span=(0, 100)\n)\n\nprint(f\"Function evaluations: {result['nfev']}\")\nprint(f\"Integration time: {result['integration_time']:.4f}s\")\n```\n:::\n\n\n### Example 4: GPU-Accelerated Neural ODE\n\n::: {#ex-neural-ode .cell execution_count=14}\n``` {.python .cell-code}\nsystem = Pendulum(m_val=1.0, l_val=0.5)\n\n# Switch to PyTorch backend for gradients\nwith system.use_backend('torch'):\n    x0 = torch.tensor([1.0, 0.0], requires_grad=True)\n    \n    result = system.integrate(\n        x0=x0,\n        method='dopri5',\n        device='cpu',  # Use 'cuda:0' for GPU\n        t_span=(0, 10)\n    )\n    \n    # Gradient computation works seamlessly\n    final_state = result['x'][-1]\n    loss = final_state.sum()\n    loss.backward()\n    \n    print(f\"Gradient w.r.t. x0: {x0.grad}\")\n```\n:::\n\n\n### Example 5: Stochastic Simulation with Monte Carlo\n\n::: {#ex-monte-carlo .cell execution_count=15}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.stochastic import get_trajectory_statistics\n\n# Create stochastic system\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\n\n# Use integrator for Monte Carlo (don't specify backend, it's handled internally)\nintegrator = SDEIntegratorFactory.for_monte_carlo(\n    sde_system, n_paths=1000\n)\n\nx0 = np.array([1.0])\nu_func = lambda t, x: np.zeros(1)\nresult = integrator.integrate_monte_carlo(\n    x0, u_func, t_span=(0, 10), n_paths=1000\n)\n\n# Extract statistics\nstats = get_trajectory_statistics(result)\n\n# Extract scalar values from arrays\nmean_final = float(stats['mean'][-1].flat[0])\nstd_final = float(stats['std'][-1].flat[0])\n\nprint(f\"Mean at t=10: {mean_final:.4f}\")\nprint(f\"Std at t=10: {std_final:.4f}\")\n```\n:::\n\n\n### Example 6: Deterministic Testing with Custom Noise\n\n::: {#ex-custom-noise .cell execution_count=16}\n``` {.python .cell-code}\nfrom cdesym.systems.base.numerical_integration.stochastic.custom_brownian import CustomBrownianPath\n\nsde_system = OrnsteinUhlenbeck(alpha=2.0, sigma=0.3)\nnw = sde_system.nw\n\n# Zero noise for deterministic testing\ndW = jnp.zeros((nw,))\nbrownian = CustomBrownianPath(t0=0.0, t1=10.0, dW=dW)\n\n# Create JAX integrator\nintegrator = SDEIntegratorFactory.create(\n    sde_system,\n    backend='jax',\n    method='Euler',\n    dt=0.01,\n    seed=SEED\n)\n\n# Integrate with custom noise\nx0 = jnp.array([0.5])\nu_func = lambda t, x: jnp.zeros(1)\nresult = integrator.integrate(\n    x0, u_func, t_span=(0, 10), brownian_path=brownian\n)\n\nprint(f\"Deterministic integration complete\")\nprint(f\"Final state: {result['x'][-1]}\")\n```\n:::\n\n\n## Integrator Selection Guide {#sec-selection-guide}\n\n### By Use Case\n\n| Use Case | Recommended Approach | Rationale |\n|----------|---------------------|-----------|\n| General ODE | `IntegratorFactory.for_production(system)` | LSODA auto-stiffness detection |\n| Neural ODE | `IntegratorFactory.for_neural_ode(system)` | Adjoint method for memory efficiency |\n| Optimization | `IntegratorFactory.for_optimization(system)` | JAX with XLA compilation |\n| High Accuracy | `IntegratorFactory.for_julia(system, 'Vern9')` | Julia 9th order methods |\n| Stiff ODE | `ScipyIntegrator(system, method='BDF')` | Backward differentiation |\n| Simple ODE | `RK4Integrator(system, dt=0.01)` | Classic fixed-step |\n| General SDE | `SDEIntegratorFactory.auto(sde_system)` | Automatic noise detection |\n| Monte Carlo | `SDEIntegratorFactory.for_monte_carlo(...)` | Parallelized trajectories |\n\n### By Backend Capabilities\n\n| Backend | ODE Integrator | SDE Integrator | Best For |\n|---------|---------------|----------------|----------|\n| NumPy | ScipyIntegrator, DiffEqPyIntegrator | DiffEqPySDEIntegrator | General purpose, highest compatibility |\n| PyTorch | TorchDiffEqIntegrator | TorchSDEIntegrator | GPU acceleration, neural networks |\n| JAX | DiffraxIntegrator | DiffraxSDEIntegrator | Optimization, functional programming |\n\n### By System Properties\n\n| System Type | Best Method | Convergence Order | Notes |\n|-------------|-------------|-------------------|-------|\n| Non-stiff | RK45, Tsit5, dopri5 | 5(4) | General purpose |\n| Stiff | BDF, Radau, Rodas5 | Variable | Implicit methods required |\n| Very stiff | BDF, LSODA | Variable | Automatic stiffness detection |\n| High accuracy | Vern9, DOP853 | 9(8) or 8(5,3) | For precision-critical applications |\n| Real-time | RK4, euler | 4 or 1 | Fixed step, predictable timing |\n| Additive noise SDE | reversible_heun, adaptive_heun | 1.0 (strong) | Exploits simplified noise structure |\n| General SDE | Euler-Maruyama | 0.5 (strong) | Robust for any noise type |\n| Diagonal noise SDE | Milstein | 1.0 (strong) | Higher order for independent channels |\n\n## Design Principles {#sec-design-principles}\n\n### Backend Abstraction\n\nAll integrators provide consistent interfaces across computational backends. Switch between NumPy, PyTorch, and JAX without changing integration code:\n\n```python\n# Same interface, different backends\nintegrator_numpy = IntegratorFactory.auto(system, backend='numpy')\nintegrator_torch = IntegratorFactory.auto(system, backend='torch')\nintegrator_jax = IntegratorFactory.auto(system, backend='jax')\n```\n\n### Factory Pattern for Complexity Management\n\nFactories encapsulate the complexity of choosing appropriate integrators:\n\n```python\n# User specifies intent, factory handles details\nIntegratorFactory.for_production(system)  # → LSODA\nIntegratorFactory.for_neural_ode(system)  # → dopri5 with adjoint\nIntegratorFactory.for_optimization(system)  # → tsit5 with JAX\n```\n\n### Unified Result Types\n\nTypedDict results provide consistent structure with IDE support:\n\n```python\nresult: IntegrationResult = integrator.integrate(...)\n# IDE knows 'success', 'nfev', 'x', 't' are available\n```\n\n### Composition Over Inheritance\n\nIntegrators compose with systems rather than inheriting deeply:\n\n```python\nclass Integrator:\n    def __init__(self, system: ContinuousSystemBase):\n        self.system = system  # Composition\n```\n\n### Built-in Performance Tracking\n\nAll integrators track statistics automatically:\n\n```python\nprint(f\"Function evaluations: {integrator._stats['total_fev']}\")\nprint(f\"Integration steps: {integrator._stats['total_steps']}\")\nprint(f\"Computation time: {integrator._stats['total_time']:.4f}s\")\n```\n\n## Key Strengths {#sec-strengths}\n\n1. **Multi-backend support**: Seamless NumPy/PyTorch/JAX switching\n2. **Extensive method library**: 40+ integration methods across ODE and SDE\n3. **Centralized method registry**: Unified method normalization, validation, and discovery\n4. **Canonical name support**: Use portable names (`euler_maruyama`, `rk45`) across backends\n5. **Intelligent factories**: Automatic method selection based on system properties\n6. **Type safety**: TypedDict results with full IDE support\n7. **GPU acceleration**: First-class support via PyTorch and JAX\n8. **XLA compilation**: Near-native performance with JAX\n9. **Julia integration**: Access to world-class DifferentialEquations.jl ecosystem\n10. **SDE support**: Comprehensive stochastic integration framework\n11. **Noise exploitation**: Automatic detection and optimization for noise structure\n12. **Monte Carlo**: Built-in multi-trajectory simulation\n13. **Custom noise**: Deterministic testing with user-provided Brownian paths\n14. **Production ready**: Professional error control and performance tracking\n\n## Summary\n\nThe ControlDESymulation integration framework provides a comprehensive, production-ready solution for numerical integration across deterministic and stochastic systems. With support for multiple backends, extensive method libraries, and intelligent automation through factory patterns, it enables efficient development of control theory and machine learning applications requiring state-of-the-art numerical integration.\n\n",
    "supporting": [
      "Integration_Framework_Architecture_files"
    ],
    "filters": [],
    "includes": {}
  }
}